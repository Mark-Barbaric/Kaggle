{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 17777,
          "databundleVersionId": 869809,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "NLP With Disaster Tweets - Pretrained BERT",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mark-Barbaric/Kaggle/blob/AP-36-Fine-tune-BERT-Pretrained-Model-for-Disaster-Tweets/disaster_tweets/NLP_With_Disaster_Tweets_Pretrained_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'nlp-getting-started:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F17777%2F869809%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240724%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240724T071108Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D20d6ecea972c4dc584acdf62d0e2e81c3e8fff0e379f16deacef5ca2594a0d7910f1eac7339d38c8bda1f99910cab2fb6a7e21948f183ae1b372f5feeabcd4c276fcb537fd4842547248104f1dec0c3b4c73580c374e73e635d2cb0f725ce1279a68d817fcaf6394ecda5d40fdf029ad537a88665ac7af93925f67ed34b5f4e4ba002dd6cc7318e4595ffa6dda69f0cedc7f2754723b5fbf4a353fc52b1986b45ffafdbcc3c692ae53e4e710dcb5014f8cc768a28dc3db05796907940507b41e95ac384039d1b7872b0cf0a9ab9051c5f89d25f2327e71dfe60f75559864e5c9238a9454c23fc58b1f1f5a93e6652154f5f1918de20b77ef2b380f520a15229c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tmE941QlGC3",
        "outputId": "55b14ee0-d725-471c-f8d4-a9c5b42d5da6"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nlp-getting-started, 607343 bytes compressed\n",
            "[==================================================] 607343 bytes downloaded\n",
            "Downloaded and uncompressed: nlp-getting-started\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disaster Tweets Using KerasNLP and Pretrained BERT"
      ],
      "metadata": {
        "id": "MDxZIDfSlGC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "id": "rThbPVQLlGC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:10:06.221186Z",
          "iopub.execute_input": "2024-07-24T07:10:06.222603Z",
          "iopub.status.idle": "2024-07-24T07:10:06.229373Z",
          "shell.execute_reply.started": "2024-07-24T07:10:06.222547Z",
          "shell.execute_reply": "2024-07-24T07:10:06.228047Z"
        },
        "trusted": true,
        "id": "HI9UWsn_lGC_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
        "print(IS_KAGGLE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T06:59:09.61581Z",
          "iopub.execute_input": "2024-07-24T06:59:09.616238Z",
          "iopub.status.idle": "2024-07-24T06:59:09.624608Z",
          "shell.execute_reply.started": "2024-07-24T06:59:09.616183Z",
          "shell.execute_reply": "2024-07-24T06:59:09.621391Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtzdeNN-lGC_",
        "outputId": "1bf8df05-5c7e-4dc2-84ba-c7eb3401c440"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this is being run in Kaggle then the dependencies will need to be installed directly into the image."
      ],
      "metadata": {
        "id": "ptmy6HyulGDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IS_KAGGLE:\n",
        "    print(\"Installing additional libs\")\n",
        "    !pip install keras-core --upgrade\n",
        "    !pip install -q keras-nlp --upgrade\n",
        "    !pip install nltk tweet-preprocessor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T06:59:09.629979Z",
          "iopub.execute_input": "2024-07-24T06:59:09.630442Z",
          "iopub.status.idle": "2024-07-24T07:01:40.798173Z",
          "shell.execute_reply.started": "2024-07-24T06:59:09.630403Z",
          "shell.execute_reply": "2024-07-24T07:01:40.796897Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuYNoneTlGDA",
        "outputId": "27696524-1518-4c85-a6ba-45656f0a835c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing additional libs\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core) (3.11.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KERAS_BACKEND'] = 'tensorflow'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:10:10.768102Z",
          "iopub.execute_input": "2024-07-24T07:10:10.768514Z",
          "iopub.status.idle": "2024-07-24T07:10:10.773607Z",
          "shell.execute_reply.started": "2024-07-24T07:10:10.76848Z",
          "shell.execute_reply": "2024-07-24T07:10:10.772545Z"
        },
        "trusted": true,
        "id": "tmkOaHQ-lGDA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lib Imports"
      ],
      "metadata": {
        "id": "GXkXyIfZlGDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_core as keras\n",
        "import keras_nlp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:10:19.183236Z",
          "iopub.execute_input": "2024-07-24T07:10:19.183735Z",
          "iopub.status.idle": "2024-07-24T07:10:35.556389Z",
          "shell.execute_reply.started": "2024-07-24T07:10:19.183699Z",
          "shell.execute_reply": "2024-07-24T07:10:35.554683Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJABUS1tlGDB",
        "outputId": "e48d98f2-b649-4887-b06b-77028e2d8777"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LLQerAnnR14",
        "outputId": "6941a015-154b-4aa3-9317-aa8f96b8d38d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ENGLISH_STOPWORDS = set(stopwords.words('english'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.118138Z",
          "iopub.status.idle": "2024-07-24T07:01:41.118506Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.118332Z",
          "shell.execute_reply": "2024-07-24T07:01:41.118347Z"
        },
        "trusted": true,
        "id": "uQ6kzSiSlGDB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local Helper Method Imports"
      ],
      "metadata": {
        "id": "g_tOZHLJlGDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding local path to sys path to add lib"
      ],
      "metadata": {
        "id": "1LJXFzA4lGDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_path = Path.cwd()\n",
        "print(f\"current_path: {current_path}\")\n",
        "sys.path.append(current_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.119817Z",
          "iopub.status.idle": "2024-07-24T07:01:41.120179Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.12001Z",
          "shell.execute_reply": "2024-07-24T07:01:41.120025Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCWEnJ1OlGDC",
        "outputId": "bfece3cf-29a4-474d-daaf-6fad582d4e3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_path: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from disaster_tweet_helpers import preprocess_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.122105Z",
          "iopub.status.idle": "2024-07-24T07:01:41.122469Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.122299Z",
          "shell.execute_reply": "2024-07-24T07:01:41.122314Z"
        },
        "trusted": true,
        "id": "XcJQbVRflGDC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocessor as tweet_preprocessor\n",
        "import string\n",
        "\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        df (_type_): _description_\n",
        "        column_name (_type_): _description_\n",
        "        stopwords (_type_): _description_\n",
        "    \"\"\"\n",
        "    def remove_punctuations(text: str):\n",
        "        for punctuation in string.punctuation:\n",
        "            text = text.replace(punctuation, '')\n",
        "        return text\n",
        "\n",
        "    text = text.lower()\n",
        "    text = remove_punctuations(text)\n",
        "    text = text.replace('\\s\\s+', ' ')\n",
        "    text = tweet_preprocessor.clean(text)\n",
        "    text = ' '.join([w for w in text.split(' ') if w not in stopwords])\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.124254Z",
          "iopub.status.idle": "2024-07-24T07:01:41.124626Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.124457Z",
          "shell.execute_reply": "2024-07-24T07:01:41.124472Z"
        },
        "trusted": true,
        "id": "XMy5LDKvlGDC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "vmI3mO8YlGDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "Y_COLUMN = 'target'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.125449Z",
          "iopub.status.idle": "2024-07-24T07:01:41.125819Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.125627Z",
          "shell.execute_reply": "2024-07-24T07:01:41.125643Z"
        },
        "trusted": true,
        "id": "qnN7vDwMlGDC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset Loading and Analysis"
      ],
      "metadata": {
        "id": "aIrejqq9lGDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Train and Test Dataset Loading"
      ],
      "metadata": {
        "id": "Y3DH_K3AlGDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DF_DIR = '/kaggle/input/nlp-getting-started/train.csv' if IS_KAGGLE else 'train.csv'\n",
        "TEST_DF_DIR = '/kaggle/input/nlp-getting-started/test.csv' if IS_KAGGLE else 'test.csv'"
      ],
      "metadata": {
        "id": "An6EwtzipGoH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(TRAIN_DF_DIR)\n",
        "test_df = pd.read_csv(TEST_DF_DIR)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.127811Z",
          "iopub.status.idle": "2024-07-24T07:01:41.128195Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.128016Z",
          "shell.execute_reply": "2024-07-24T07:01:41.128032Z"
        },
        "trusted": true,
        "id": "783RVxf_lGDD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.129512Z",
          "iopub.status.idle": "2024-07-24T07:01:41.129883Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.129687Z",
          "shell.execute_reply": "2024-07-24T07:01:41.129701Z"
        },
        "trusted": true,
        "id": "6OqVlm06lGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.130878Z",
          "iopub.status.idle": "2024-07-24T07:01:41.131233Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.131062Z",
          "shell.execute_reply": "2024-07-24T07:01:41.131077Z"
        },
        "trusted": true,
        "id": "zaJc7Cl1lGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 EDA"
      ],
      "metadata": {
        "id": "iABhKyfslGDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['preprocessed_text'] = train_df['text'].apply(lambda x: preprocess_text(x, ENGLISH_STOPWORDS))\n",
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.132575Z",
          "iopub.status.idle": "2024-07-24T07:01:41.13295Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.132747Z",
          "shell.execute_reply": "2024-07-24T07:01:41.132761Z"
        },
        "trusted": true,
        "id": "gaGgsgcnlGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['length'] = train_df['preprocessed_text'].apply(lambda x: len(x.split(' ')))\n",
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.134438Z",
          "iopub.status.idle": "2024-07-24T07:01:41.134944Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.134676Z",
          "shell.execute_reply": "2024-07-24T07:01:41.134699Z"
        },
        "trusted": true,
        "id": "sTJlA3WSlGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['length'].hist(bins=20, figsize=(6, 6))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.136647Z",
          "iopub.status.idle": "2024-07-24T07:01:41.137135Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.13689Z",
          "shell.execute_reply": "2024-07-24T07:01:41.136911Z"
        },
        "trusted": true,
        "id": "VgBtyoZDlGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['length'].describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.138429Z",
          "iopub.status.idle": "2024-07-24T07:01:41.138932Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.138666Z",
          "shell.execute_reply": "2024-07-24T07:01:41.138687Z"
        },
        "trusted": true,
        "id": "8r7nzLkSlGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df['preprocessed_text']\n",
        "y = train_df[Y_COLUMN]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.140491Z",
          "iopub.status.idle": "2024-07-24T07:01:41.140997Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.140732Z",
          "shell.execute_reply": "2024-07-24T07:01:41.140752Z"
        },
        "trusted": true,
        "id": "te3fErbqlGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Loading and Fine Tuning"
      ],
      "metadata": {
        "id": "G-2AFgeTlGDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preset = 'distil_bert_base_en_uncased'\n",
        "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset, num_classes=2)\n",
        "classifier.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.143317Z",
          "iopub.status.idle": "2024-07-24T07:01:41.143844Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.143576Z",
          "shell.execute_reply": "2024-07-24T07:01:41.143597Z"
        },
        "trusted": true,
        "id": "Ueu0JlinlGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Model Fine Tuning"
      ],
      "metadata": {
        "id": "bGwnTtZ0lGDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = classifier.fit(x=X_train,\n",
        "                         y=y_train,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         epochs=2,\n",
        "                         validation_data=(X_test, y_test),\n",
        "                         verbose=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T07:01:41.144973Z",
          "iopub.status.idle": "2024-07-24T07:01:41.145451Z",
          "shell.execute_reply.started": "2024-07-24T07:01:41.145202Z",
          "shell.execute_reply": "2024-07-24T07:01:41.145221Z"
        },
        "trusted": true,
        "id": "OVkzd5aXlGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeKkXjI-lGDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}